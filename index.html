<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-STG67VH2DD"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-STG67VH2DD');
</script>
<meta name=viewport content="width=800">
<meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
<style type="text/css">
  /* Color scheme stolen from Sergey Karayev */
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th,tr,p,a {
  font-family: 'Lato', Verdana, Helvetica, sans-serif;
  font-size: 14px
  }
  strong {
  font-family: 'Lato', Verdana, Helvetica, sans-serif;
  font-size: 14px;
  }
  heading {
  font-family: 'Lato', Verdana, Helvetica, sans-serif;
  font-size: 22px;
  }
  heading2 {
  font-family: 'Lato', Verdana, Helvetica, sans-serif;
  font-size: 18px;
  }
  papertitle {
  font-family: 'Lato', Verdana, Helvetica, sans-serif;
  font-size: 14px;
  font-weight: 700
  }
  name {
  font-family: 'Lato', Verdana, Helvetica, sans-serif;
  font-size: 32px;
  }
  award {
  color: red;
  font-weight: 700;
  }
  .paperthumbcol
  {
  vertical-align: middle;
  width: 27%;
  }
  .paperthumb
  {
  width: 200px;
  }
  .papertextcol
  {
  vertical-align: middle;
  width: 75%;
  }
  .one
  {
  width: 160px;
  height: 160px;
  position: relative;
  }
  .two
  {
  width: 160px;
  height: 160px;
  position: absolute;
  transition: opacity .2s ease-in-out;
  -moz-transition: opacity .2s ease-in-out;
  -webkit-transition: opacity .2s ease-in-out;
  }
  .fade {
   transition: opacity .2s ease-in-out;
   -moz-transition: opacity .2s ease-in-out;
   -webkit-transition: opacity .2s ease-in-out;
  }
  span.highlight {
      background-color: #ffffd0;
  }
</style>
<!-- javascript -->
<script>
function hidebib(paperid)
{
  var block = document.getElementById(paperid);
  block.querySelector("#bibtex").style.display = 'none' ;
}
function toggleblock(blockid)
{
  var block = document.getElementById(blockid) ;
  if (block.style.display == 'none') {
      block.style.display = 'block' ;
  } else {
      block.style.display = 'none' ;
  }
}
function toggleAbsVsBib(paperid)
{
  var block = document.getElementById(paperid);
  if (block.querySelector("#abstract").style.display == 'none'){
  	block.querySelector("#abstract").style.display = 'block';
  	block.querySelector("#bibtex").style.display = 'none';
  	block.querySelector("#toggle").innerText = 'show bibtex';
  } else {
  	block.querySelector("#abstract").style.display = 'none';
  	block.querySelector("#bibtex").style.display = 'block';
  	block.querySelector("#toggle").innerText = 'show abstract';
  }
}
function copybib(paperid) {
  /* Get the text field */
  var block = document.getElementById(paperid) ;
  var bib = block.getElementsByTagName('pre') ;
  var copyText = bib[0].firstChild.nodeValue ;

  /* Copy the text inside the text field */
  navigator.clipboard.writeText(copyText);
}
</script>
<link rel="icon" type="image/png" href="assets/images/mimi.png">
<title>Aaditya K. Singh</title>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>
<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
<tr><td>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
<tr><td width="67%" valign="middle">
<p align="center">
<name>Aaditya K. Singh</name>
</p><p>
I'm currently doing a PhD at the Gatsby Computational Neuroscience Unit in London, where I'm lucky to be co-supervised by <a href="https://www.saxelab.org/people/andrewsaxe/">Andrew Saxe</a> and <a href="https://fh295.github.io/">Felix Hill</a>. I'm interested in building and better understanding safe AIs that can push the frontiers of human reasoning and help us do science as safe and helpful assistants. During my PhD, I've had the pleasure of interning at <a href="https://ai.meta.com/research/">Meta FAIR</a> (with <a href="http://www.arimorcos.com/">Ari Morcos</a> and then on the <a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">Llama 3/3.1 team</a>) and <a href="https://deepmind.google/">Google DeepMind</a> (on the Grounded Language Agents team).
</p><p>
Before my PhD, I did my undergrad and master's at MIT, where I double majored in Computer Science and Neuroscience, and was supervised by <a href="https://people.csail.mit.edu/boris/boris.html">Boris Katz</a> and <a href="https://deepmind.com/">Ila Fiete</a>. I also did a smattering of internships in <a href="https://www.metsci.com/">applied math</a>, <a href="https://www.nrl.navy.mil/Our-Work/Areas-of-Research/Computational-Physics-Fluid-Dynamics/">computational physics</a>, <a href="https://www.featurex.ai/">software engineering</a>, <a href="https://www.orbitalinsight.com/">computer vision</a>, and <a href="https://www.citadelsecurities.com/">quantitative research</a> on my path to figuring out what I wanted to do.
</p><p align=center>
<a href="mailto:aaditya.singh.21@ucl.ac.uk">Email</a> &nbsp|&nbsp
<a href="https://twitter.com/Aaditya6284">Twitter</a> &nbsp|&nbsp
<a href="assets/AadityaSinghCV.pdf">CV</a> &nbsp|&nbsp
<a href="https://scholar.google.com/citations?user=9OPKqmMAAAAJ&hl=en">Scholar</a> &nbsp|&nbsp
<a href="https://github.com/aadityasingh">GitHub</a>
</p></td>
<td width="33%">
<img src="assets/images/Aaditya167KB.jpeg" width="240"> <!-- should be 280x280 -->
</td></tr></table>

<!-- RESEARCH SECTION -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
<tr> <td width="100%" valign="middle">
<heading>Research</heading> <p>
<!-- I'm broadly interested in reinforcement and deep learning. Lately, I've been especially interested in the efficient training of agents that can meaningfully interact and collaborate with humans. My <a href="downloads/DJStrouseThesis.pdf">PhD thesis ("Optimization of MILES")</a> focused on applications of the information bottleneck (IB) across supervised, unsupervised, and reinforcement learning, and definitely not on collecting airline miles. In past lives, I've also worked on quantum information theory and computational neuroscience. -->
</p> </td> </tr> </table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

<tr><td width="27%">
<heading2><i>In-Context Learning</i></heading2>
</td/></tr>

<!-- project begin -->
<tr id="singh2024ih"><td class="paperthumbcol">
<!-- project symbol -->
<img src="./assets/images/InductionHeads.png" class="paperthumb">
</td><td class="papertextcol">
<!-- title -->
<a href="https://arxiv.org/abs/2404.07129">
<papertitle>What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation</papertitle></a><br>
<!-- authors -->
<strong>Aaditya K. Singh</strong>,
<a href="https://scholar.google.com/citations?user=pPVXrTYAAAAJ&hl=en">Ted Moskovitz</a>,
<a href="https://scholar.google.co.uk/citations?user=4HLUnhIAAAAJ&hl=en">Felix Hill</a>,
<a href="https://scholar.google.com/citations?user=bXOt49QAAAAJ&hl=en">Stephanie C.Y. Chan*</a>,
<a href="https://scholar.google.co.uk/citations?user=h0Al1fcAAAAJ&hl=en">Andrew M. Saxe*</a><br>
<!-- publication status -->
<em>International Conference on Machine Learning (ICML)</em>, 2024 <award>(Spotlight)</award><br>
<!-- links -->
<a href="https://arxiv.org/abs/2107.14226">arxiv</a> |
<a href="https://github.com/aadityasingh/icl-dynamics">github</a> |
<a href="https://x.com/Aaditya6284/status/1778442926688813421">tweet</a> |
<a href="javascript:toggleAbsVsBib('singh2024ih')" id="toggle">show bibtex</a>
<p></p>
<!-- project description -->
<p id="abstract">Induction heads are thought to be responsible for much of in-context learning. We take inspiration from optogenetics in neuroscience to introduce a framework for measuring and manipulating activations in a deep network throughout training. We introduce the method of <i>clamping</i> to better understand what gives rise to the phase change characteristic of induction circuit formation, finding that the interaction between three smoothly evolving sub-circuits yields this sudden drop in the loss.</p>
<!-- bibtex -->
<p id="bibtex"><tt>
@misc{singh2024needsrightinductionhead, <br>
&nbsp; title={What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation}, <br>
&nbsp; author={Aaditya K. Singh and Ted Moskovitz and Felix Hill and Stephanie C. Y. Chan and Andrew M. Saxe}, <br>
&nbsp; year={2024}, <br>
&nbsp; eprint={2404.07129}, <br>
&nbsp; archivePrefix={arXiv}, <br>
&nbsp; primaryClass={cs.LG}, <br>
&nbsp; url={https://arxiv.org/abs/2404.07129}, <br>
}
</tt></p>
<script xml:space="preserve" language="JavaScript">
hidebib('singh2024ih');
</script>
</td></tr>
<!-- project end -->

<!-- project begin -->
<tr id="singh2023transience"><td class="paperthumbcol">
<!-- project symbol -->
<img src="./assets/images/Transience.png" class="paperthumb">
</td><td class="papertextcol">
<!-- title -->
<a href="https://arxiv.org/abs/2404.07129">
<papertitle>The transient nature of emergent in-context learning in transformers</papertitle></a><br>
<!-- authors -->
<strong>Aaditya K. Singh*</strong>,
<a href="https://scholar.google.com/citations?user=bXOt49QAAAAJ&hl=en">Stephanie C.Y. Chan*</a>,
<a href="https://scholar.google.com/citations?user=pPVXrTYAAAAJ&hl=en">Ted Moskovitz</a>,
<a href="https://scholar.google.com/citations?user=OSg3D9MAAAAJ&hl=en">Erin Grant</a>,
<a href="https://scholar.google.co.uk/citations?user=h0Al1fcAAAAJ&hl=en">Andrew M. Saxe**</a>,
<a href="https://scholar.google.co.uk/citations?user=4HLUnhIAAAAJ&hl=en">Felix Hill**</a><br>
<!-- publication status -->
<em>Neural Information Processing Systems (NeurIPS)</em>, 2023<br>
<!-- links -->
<a href="https://arxiv.org/abs/2311.08360">arxiv</a> |
<a href="https://neurips.cc/virtual/2023/poster/71806">neurips</a> |
<a href="https://github.com/aadityasingh/icl-dynamics">github</a> |
<a href="https://x.com/Aaditya6284/status/1724645286495281423">tweet</a> |
<a href="javascript:toggleAbsVsBib('singh2023transience')" id="toggle">show bibtex</a>
<p></p>
<!-- project description -->
<p id="abstract">We train transformers on synthetic data designed so that both in-context learning (ICL) and in-weights learning (IWL) strategies can lead to correct predictions. We find that ICL is often <i>transient</i>, meaning it first emerges, then disappears and gives way to IWL, all while the training loss decreases, indicating an asymptotic preference for IWL. We find that L2 regularization may offer a path to more persistent ICL that removes the need for early stopping based on ICL-style validation tasks. Finally, we present initial evidence that ICL transience may be caused by competition between ICL and IWL circuits.</p>
<!-- bibtex -->
<p id="bibtex"><tt>
@inproceedings{singh2023transience, <br>
&nbsp; author = {Singh, Aaditya and Chan, Stephanie and Moskovitz, Ted and Grant, Erin and Saxe, Andrew and Hill, Felix}, <br>
&nbsp; booktitle = {Advances in Neural Information Processing Systems}, <br>
&nbsp; editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine}, <br>
&nbsp; pages = {27801--27819}, <br>
&nbsp; publisher = {Curran Associates, Inc.}, <br>
&nbsp; title = {The Transient Nature of Emergent In-Context Learning in Transformers}, <br>
&nbsp; url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/58692a1701314e09cbd7a5f5f3871cc9-Paper-Conference.pdf}, <br>
&nbsp; volume = {36}, <br>
&nbsp; year = {2023} <br>
}
</tt></p>
<script xml:space="preserve" language="JavaScript">
hidebib('singh2023transience');
</script>
<p><i>Reproduced by</i> <a href="https://arxiv.org/abs/2406.00053">this recent paper</a> in naturalistic settings, and <a href="https://arxiv.org/abs/2406.02550">this other paper</a> in a different synthetic setting.</p>
</td></tr>
<!-- project end -->

<!-- project begin -->
<tr id="chan2022data"><td class="paperthumbcol">
<!-- project symbol -->
<img src="./assets/images/Chan2022.png" class="paperthumb">
</td><td class="papertextcol">
<!-- title -->
<a href="https://arxiv.org/abs/2205.05055">
<papertitle>Data distributional properties drive emergent in-context learning in transformers</papertitle></a><br>
<!-- authors -->
<a href="https://scholar.google.com/citations?user=bXOt49QAAAAJ&hl=en">Stephanie C.Y. Chan</a>,
<a href="https://scholar.google.com/citations?user=evIkDWoAAAAJ&hl=en">Adam Santoro</a>,
<a href="https://scholar.google.com/citations?user=_N44XxAAAAAJ&hl=en">Andrew Kyle Lampinen</a>,
<a href="https://scholar.google.com/citations?user=YizAq4gAAAAJ&hl=en">Jane X. Wang</a>,
<strong>Aaditya K. Singh</strong>,
<a href="https://scholar.google.com/citations?user=cw_TpTcAAAAJ&hl=en">Pierre H. Richemond</a>,
<a href="https://scholar.google.com/citations?user=ht_psVIAAAAJ&hl=en">Jay McClelland</a>,
<a href="https://scholar.google.co.uk/citations?user=4HLUnhIAAAAJ&hl=en">Felix Hill</a><br>
<!-- publication status -->
<em>Neural Information Processing Systems (NeurIPS)</em>, 2022 <award>(Oral)</award><br>
<!-- links -->
<a href="https://arxiv.org/abs/2205.05055">arxiv</a> |
<a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/77c6ccacfd9962e2307fc64680fc5ace-Abstract-Conference.html">neurips</a> | 
<a href="https://github.com/google-deepmind/emergent_in_context_learning">github</a> |
<a href="https://twitter.com/scychan_brains/status/1526514761579614209">tweet</a> |
<a href="javascript:toggleAbsVsBib('chan2022data')" id="toggle">show bibtex</a>
<p></p>
<!-- project description -->
<p id="abstract">Large transformer-based models are able to perform in-context few-shot learning, without being explicitly trained for it. We find that in-context learning (ICL) emerges when the training data exhibits particular distributional properties such as burstiness (items appear in clusters rather than being uniformly distributed over time) and having large numbers of rarely occurring classes. We found that in-context learning typically trades off against more conventional weight-based learning, but that the two modes of learning could co-exist in a single model when it was trained on data following a skewed Zipfian distribution (a common property of naturalistic data, including language).</p>
<!-- bibtex -->
<p id="bibtex"><tt>
@inproceedings{chan2022icldata, <br>
&nbsp; author = {Chan, Stephanie and Santoro, Adam and Lampinen, Andrew and Wang, Jane and Singh, Aaditya and Richemond, Pierre and McClelland, James and Hill, Felix}, <br>
&nbsp; booktitle = {Advances in Neural Information Processing Systems}, <br>
&nbsp; editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh}, <br>
&nbsp; pages = {18878--18891}, <br>
&nbsp; publisher = {Curran Associates, Inc.}, <br>
&nbsp; title = {Data Distributional Properties Drive Emergent In-Context Learning in Transformers}, <br>
&nbsp; url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/77c6ccacfd9962e2307fc64680fc5ace-Paper-Conference.pdf}, <br>
&nbsp; volume = {35}, <br>
&nbsp; year = {2022} <br>
}
</tt></p>
<script xml:space="preserve" language="JavaScript">
hidebib('chan2022data');
</script>
<p><i>Our insights were used by</i> <a href="https://arxiv.org/abs/2312.03801">this recent paper</a> to elicit ICL in RL settings.</p>
</td></tr>
<!-- project end -->

<tr><td width="27%">
<heading2><i>Large Language Models</i></heading2>
</td/></tr>

<!-- project begin -->
<tr id="llama3"><td class="paperthumbcol">
<!-- project symbol -->
<img src="./assets/images/Llama3.png" class="paperthumb">
</td><td class="papertextcol">
<!-- title -->
<a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">
<papertitle>The Llama 3 herd of models</papertitle></a><br>
<!-- authors -->
Llama team, AI@Meta, Contributors: <strong>Aaditya K. Singh</strong>, ...<br>
<!-- links -->
<a href="https://arxiv.org/abs/2407.21783">arxiv</a> |
<a href="https://github.com/meta-llama/llama-models">github</a> |
<a href="https://x.com/Aaditya6284/status/1815871036669452689">tweet</a> |
<a href="javascript:toggleAbsVsBib('llama3')" id="toggle">show bibtex</a>
<p></p>
<p id="abstract">Llama 3 is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens.</p>
<p id="bibtex"><tt>
@misc{llama3herdmodels, <br>
&nbsp; title={The Llama 3 Herd of Models},  <br>
&nbsp; author={LLama team AI@Meta}, <br>
&nbsp; year={2024}, <br>
&nbsp; eprint={2407.21783}, <br>
&nbsp; archivePrefix={arXiv}, <br>
&nbsp; primaryClass={cs.AI}, <br>
&nbsp; url={https://arxiv.org/abs/2407.21783},  <br>
}
</tt></p>
<script xml:space="preserve" language="JavaScript">
hidebib('llama3');
</script>
<p><i>My contributions:</i> Math pretraining data (S3.1.1), Scaling laws (S3.2.1), and Evals (S5).</p>
</td></tr>
<!-- project end -->

<!-- project begin -->
<tr id="singh2024tokenization"><td class="paperthumbcol">
<!-- project symbol -->
<img src="./assets/images/Tokenization.png" class="paperthumb">
</td><td class="papertextcol">
<!-- title -->
<a href="https://arxiv.org/abs/2402.14903">
<papertitle>Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs</papertitle></a><br>
<!-- authors -->
<strong>Aaditya K. Singh</strong>,
<a href="https://scholar.google.com/citations?user=K8E0T7MAAAAJ&hl=en">DJ Strouse</a><br>
<!-- publication status -->
<em>arxiv</em>, 2024<br>
<!-- links -->
<a href="https://arxiv.org/abs/2402.14903">arxiv</a> |
<a href="https://github.com/aadityasingh/TokenizationCounts">github</a> |
<a href="https://twitter.com/Aaditya6284/status/1762558428617007569">tweet</a> |
<a href="javascript:toggleAbsVsBib('singh2024tokenization')" id="toggle">show bibtex</a>
<p></p>
<!-- project description -->
<p id="abstract">Through a series of carefully controlled, inference-time experiments, we find evidence of strong (scale-dependent) number tokenization-induced inductive biases in numerical reasoning in frontier LLMs. Specifically, we demonstrate that GPT-3.5 and GPT-4 models show largely improved performance when using right-to-left (as opposed to default left-to-right) number tokenization. Furthermore, we find that model errors when using standard left-to-right tokenization follow stereotyped error patterns, suggesting that model computations are systematic rather than approximate. These effects are weaker in larger models (GPT-4) yet stronger in newer, smaller models (GPT 4 Turbo).</p>
<!-- bibtex -->
<p id="bibtex"><tt>
@misc{singh2024tokenization, <br>
&nbsp; title={Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs},  <br>
&nbsp; author={Aaditya K. Singh and DJ Strouse}, <br>
&nbsp; year={2024}, <br>
&nbsp; eprint={2402.14903}, <br>
&nbsp; archivePrefix={arXiv}, <br>
&nbsp; primaryClass={cs.CL}, <br>
&nbsp; url={https://arxiv.org/abs/2402.14903},  <br>
}
</tt></p>
<script xml:space="preserve" language="JavaScript">
hidebib('singh2024tokenization');
</script>
<p><i>Reproduced by</i> <a href="https://www.beren.io/2024-07-07-Right-to-Left-Integer-Tokenization/">this recent blog post</a> in newer models (e.g., Llama 3). <a href="https://www.anthropic.com/news/claude-3-family">Claude 3</a>, released after our work and with SOTA math capabilities, also notably <a href="https://x.com/javirandor/status/1768616042224374133?s=20">uses R2L tokenization</a>.</p>
</td></tr>
<!-- project end -->

<!-- project begin -->
<tr id="singh2024contamination"><td class="paperthumbcol">
<!-- project symbol -->
<img src="./assets/images/Contamination.png" class="paperthumb">
</td><td class="papertextcol">
<!-- title -->
<a href="https://arxiv.org/abs/2411.03923">
<papertitle>Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?</papertitle></a><br>
<!-- authors -->
<strong>Aaditya K. Singh*</strong>,
<a href="https://scholar.google.com/citations?user=FSP9yGQAAAAJ&hl=en">Muhammed Yusuf Kocyigit*</a>,
Andrew Poulton,
<a href="https://scholar.google.com/citations?user=2jch48oAAAAJ&hl=en">David Esiobu</a>,
<a href="https://scholar.google.co.uk/citations?user=8SK2fPAAAAAJ&hl=en">Maria Lomeli</a>,
<a href="https://scholar.google.com/citations?user=fTMFXPIAAAAJ&hl=en">Gergely Szilvasy</a>,
<a href="https://scholar.google.com/citations?user=tAtSMTcAAAAJ&hl=nl">Dieuwke Hupkes</a><br>
<!-- publication status -->
<em>In submission</em>, 2024<br>
<!-- links -->
<a href="https://arxiv.org/abs/2411.03923">arxiv</a> |
<!-- <a href="https://x.com/louvishh/status/1802744614870270012">tweet</a> | -->
<a href="javascript:toggleAbsVsBib('singh2024contamination')" id="toggle">show bibtex</a>
<p></p>
<!-- project description -->
<p id="abstract">Evaluation data contamination is defined as the presence of benchmark samples in pretraining data, and the subsequent effects on performance. While easily understood intuitively, it is surprisingly difficult to define precisely which samples should be considered contaminated and, consequently, how it impacts benchmark scores. We propose that these questions should be addressed together and that contamination metrics can be assessed based on whether models benefit from the examples they mark contaminated. We propose a novel analysis method called ConTAM, and show with a large scale survey that our method can be used to better understand evaluation data contamination and its effects. We find that contamination effects may be more prevalent than reported in recent LLM releases and can be scale-dependent. We also find that considering only the longest contaminated substring provides a better signal than other n-gram based metrics.</p> 
<!-- bibtex -->
<p id="bibtex"><tt>
@misc{singh2024evaluationdatacontaminationllms, <br>
      title={Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?},  <br>
      author={Aaditya K. Singh and Muhammed Yusuf Kocyigit and Andrew Poulton and David Esiobu and Maria Lomeli and Gergely Szilvasy and Dieuwke Hupkes}, <br>
      year={2024}, <br>
      eprint={2411.03923}, <br>
      archivePrefix={arXiv}, <br>
      primaryClass={cs.CL}, <br>
      url={https://arxiv.org/abs/2411.03923},  <br>
}
</tt></p>
<script xml:space="preserve" language="JavaScript">
hidebib('singh2024contamination');
</script>
</td></tr>
<!-- project end -->


<!-- project begin -->
<tr id="moskovitz2023crlhf"><td class="paperthumbcol">
<!-- project symbol -->
<img src="./assets/images/CRLHF.png" class="paperthumb">
</td><td class="papertextcol">
<!-- title -->
<a href="https://arxiv.org/abs/2310.04373">
<papertitle>Confronting reward model overoptimization with constrained RLHF</papertitle></a><br>
<!-- authors -->
<a href="https://scholar.google.com/citations?user=pPVXrTYAAAAJ&hl=en">Ted Moskovitz</a>,
<strong>Aaditya K. Singh</strong>,
<a href="https://scholar.google.com/citations?user=K8E0T7MAAAAJ&hl=en">DJ Strouse</a>,

<a href="https://scholar.google.com/citations?user=0DpK1EMAAAAJ&hl=en">Tuomas Sandholm</a>,
<a href="https://scholar.google.co.uk/citations?user=ITZ1e7MAAAAJ&hl=en">Ruslan Salakhutdinov</a>,
<a href="https://scholar.google.com/citations?user=UgHB5oAAAAAJ&hl=en">Anca D. Dragan</a>,
<a href="https://scholar.google.com/citations?user=iEFL4-YAAAAJ&hl=en">Stephen McAleer</a><br>
<!-- publication status -->
<em>International Conference on Learning Representations</em>, 2024 <award>(Spotlight)</award><br>
<!-- links -->
<a href="https://arxiv.org/abs/2310.04373">arxiv</a> |
<a href="https://openreview.net/forum?id=gkfUvn0fLU">openreview</a> |
<a href="https://x.com/ted_moskovitz/status/1714272352400502942">tweet</a> |
<a href="javascript:toggleAbsVsBib('moskovitz2023crlhf')" id="toggle">show bibtex</a>
<p></p>
<!-- project description -->
<p id="abstract"> Optimizing large language models to align with human preferences via reinforcement learning from human feedback can offer suffer from overoptimization. Furthermore, human preferences are often multi-faceted, requiring many sub-components. In this work, we study overoptimization in composite RMs showing that correlation between component RMs has a significant effect. We then introduce an approach to circumvent this issue using constrained reinforcement learning as a means of preventing the agent from exceeding each RM's threshold of usefulness. Our method addresses the problem of weighting component RMs by learning dynamic weights, naturally expressed by Lagrange multipliers. As a result, each RM stays within the range at which it is an effective proxy, improving evaluation performance.</p> 
<!-- bibtex -->
<p id="bibtex"><tt>
@misc{moskovitz2023confrontingrewardmodeloveroptimization, <br>
&nbsp; title={Confronting Reward Model Overoptimization with Constrained RLHF},  <br>
&nbsp; author={Ted Moskovitz and Aaditya K. Singh and DJ Strouse and Tuomas Sandholm and Ruslan Salakhutdinov and Anca D. Dragan and Stephen McAleer}, <br>
&nbsp; year={2023}, <br>
&nbsp; eprint={2310.04373}, <br>
&nbsp; archivePrefix={arXiv}, <br>
&nbsp; primaryClass={cs.LG}, <br>
&nbsp; url={https://arxiv.org/abs/2310.04373},  <br>
}
</tt></p>
<script xml:space="preserve" language="JavaScript">
hidebib('moskovitz2023crlhf');
</script>
</td></tr>
<!-- project end -->

<!-- project begin -->
<tr id="madaan2024variance"><td class="paperthumbcol">
<!-- project symbol -->
<img src="./assets/images/Variance.png" class="paperthumb">
</td><td class="papertextcol">
<!-- title -->
<a href="https://arxiv.org/abs/2406.10229v1">
<papertitle>Quantifying Variance in Evaluation Benchmarks</papertitle></a><br>
<!-- authors -->
<a href="https://scholar.google.com/citations?user=zc2WyXkAAAAJ&hl=en">Lovish Madaan</a>,
<strong>Aaditya K. Singh</strong>,
<a href="https://scholar.google.com/citations?user=6tMEGz8AAAAJ&hl=en">Rylan Schaeffer</a>,
Andrew Poulton,
<a href="https://scholar.google.com/citations?user=EaaOeJwAAAAJ&hl=en">Sanmi Koyejo</a>,
<a href="https://www.semanticscholar.org/author/Pontus-Stenetorp/1918552">Pontus Stenetorp</a>,
<a href="https://scholar.google.com/citations?user=CWOixywAAAAJ&hl=en">Sharan Narang</a>,
<a href="https://scholar.google.com/citations?user=tAtSMTcAAAAJ&hl=nl">Dieuwke Hupkes</a><br>
<!-- publication status -->
<em>In submission</em>, 2024<br>
<!-- links -->
<a href="https://arxiv.org/abs/2406.10229v1">arxiv</a> |
<a href="https://x.com/louvishh/status/1802744614870270012">tweet</a> |
<a href="javascript:toggleAbsVsBib('madaan2024variance')" id="toggle">show bibtex</a>
<p></p>
<!-- project description -->
<p id="abstract">We quantify variance in evaluation benchmarks through a range of metrics, including difference in performance across ten 7B 210B token runs with different initializations. We find that continuous metrics often show less variance (higher signal-to-noise), suggesting they may be more useful when doing pretraining ablations, especially at smaller compute scales. Furthermore, we find that methods from human testing (e.g., item analysis or item response theory) are not effective at reducing variance.</p> 
<!-- bibtex -->
<p id="bibtex"><tt>
@misc{madaan2024variance, <br>
&nbsp; title={Quantifying Variance in Evaluation Benchmarks},  <br>
&nbsp; author={Lovish Madaan and Aaditya K. Singh and Rylan Schaeffer and Andrew Poulton and Sanmi Koyejo and Pontus Stenetorp and Sharan Narang and Dieuwke Hupkes}, <br>
&nbsp; year={2024}, <br>
&nbsp; eprint={2406.10229}, <br>
&nbsp; archivePrefix={arXiv}, <br>
&nbsp; primaryClass={cs.LG}, <br>
&nbsp; url={https://arxiv.org/abs/2406.10229},  <br>
}
</tt></p>
<script xml:space="preserve" language="JavaScript">
hidebib('madaan2024variance');
</script>
</td></tr>
<!-- project end -->

<!-- project begin -->
<tr id="singh2024brevity"><td class="paperthumbcol">
<!-- project symbol -->
<img src="./assets/images/LengthPruning.png" class="paperthumb">
</td><td class="papertextcol">
<!-- title -->
<a href="https://arxiv.org/abs/2407.00434">
<papertitle>Brevity is the soul of wit: pruning long files for code generation</papertitle></a><br>
<!-- authors -->
<strong>Aaditya K. Singh</strong>,
<a href="https://scholar.google.com/citations?user=KK6Yj4IAAAAJ&hl=en">Yu Yang</a>,
<a href="https://scholar.google.com/citations?user=B8WLbLsAAAAJ&hl=en">Kushal Tirumala</a>,
<a href="https://scholar.google.ca/citations?user=y_cwSKAAAAAJ&hl=en">Mostafa Elhoushi</a>,
<a href="https://scholar.google.com/citations?user=v-A_7UsAAAAJ&hl=en">Ari S. Morcos</a><br>
<!-- publication status -->
<em>Data-centric Machine Learning Research Workshop @ ICML</em>, 2024<br>
<!-- links -->
<a href="https://arxiv.org/abs/2407.00434">arxiv</a> |
<a href="https://twitter.com/Aaditya6284/status/1808167795806314805">tweet</a> |
<a href="javascript:toggleAbsVsBib('singh2024brevity')" id="toggle">show bibtex</a>
<p></p>
<!-- project description -->
<p id="abstract">Longer files are often conflated with "higher quality" data. This breaks down for code! The longest Python files in the public Stack dataset are often nonsensical, yet make up a disproportionate amount of tokens (2% of files make up 20% of tokens). We provide qualitative and quantitative evidence for this, ending with the causal experiment: pruning these files leads to modest improvements in efficiency and/or performance at small compute scales. As compute is scaled up, benefits diminish, as seen in <a href="https://arxiv.org/abs/2404.07177">related work</a>.</p> 
<!-- bibtex -->
<p id="bibtex"><tt>
@misc{singh2024brevity, <br>
&nbsp; title={Brevity is the soul of wit: Pruning long files for code generation},  <br>
&nbsp; author={Aaditya K. Singh and Yu Yang and Kushal Tirumala and Mostafa Elhoushi and Ari S. Morcos}, <br>
&nbsp; year={2024}, <br>
&nbsp; eprint={2407.00434}, <br>
&nbsp; archivePrefix={arXiv}, <br>
&nbsp; primaryClass={cs.CL}, <br>
&nbsp; url={https://arxiv.org/abs/2407.00434},  <br>
}
</tt></p>
<script xml:space="preserve" language="JavaScript">
hidebib('singh2024brevity');
</script>
</td></tr>
<!-- project end -->

<!-- project begin -->
<tr id="yang2023scip"><td class="paperthumbcol">
<!-- project symbol -->
<img src="./assets/images/SCIP.png" class="paperthumb">
</td><td class="papertextcol">
<!-- title -->
<a href="https://arxiv.org/abs/2312.02418">
<papertitle>Decoding data quality via synthetic corruptions: embedding-guided pruning of code data</papertitle></a><br>
<!-- authors -->
<a href="https://scholar.google.com/citations?user=KK6Yj4IAAAAJ&hl=en">Yu Yang</a>,
<strong>Aaditya K. Singh</strong>,
<a href="https://scholar.google.ca/citations?user=y_cwSKAAAAAJ&hl=en">Mostafa Elhoushi</a>,
<a href="https://scholar.google.ca/citations?user=z7ExCjYAAAAJ&hl=en">Anas Mahmoud</a>,
<a href="https://scholar.google.com/citations?user=B8WLbLsAAAAJ&hl=en">Kushal Tirumala</a>,
Fabian Gloeckle,
<a href="https://scholar.google.com/citations?user=CrSf2CQAAAAJ&hl=en">Baptiste Rozi&#232;re</a>,
<a href="https://scholar.google.com/citations?user=S1szbyAAAAAJ&hl=en">Carole-Jean Wu</a>,
<a href="https://scholar.google.com/citations?user=v-A_7UsAAAAJ&hl=en">Ari S. Morcos</a>,
<a href="https://scholar.google.com/citations?user=B8WLbLsAAAAJ&hl=en">Newsha Ardalani</a><br>
<!-- publication status -->
<em>Efficient Natural Language and Speech Processing Workshop @ NeurIPS</em>, 2023 <award>(Oral)</award><br>
<!-- links -->
<a href="https://arxiv.org/abs/2312.02418">arxiv</a> |
<a href="https://twitter.com/YUYANG_UCLA/status/1735711590434082946">tweet</a> |
<a href="javascript:toggleAbsVsBib('yang2023scip')" id="toggle">show bibtex</a>
<p></p>
<!-- project description -->
<p id="abstract">Code datasets, often collected from diverse and uncontrolled sources such as GitHub, potentially suffer from quality issues, thereby affecting the performance and training efficiency of Large Language Models (LLMs) optimized for code generation. First, we explore features of "low-quality" code in embedding space, through the use of synthetic corruptions. Armed with this knowledge, we devise novel pruning metrics that operate in embedding space to identify and remove low-quality entries in the Stack dataset. We demonstrate the benefits of this synthetic corruption informed pruning (SCIP) approach on the well-established HumanEval and MBPP benchmarks, outperforming existing embedding-based methods.</p> 
<!-- bibtex -->
<p id="bibtex"><tt>
@misc{yang2023scip, <br>
&nbsp; title={Decoding Data Quality via Synthetic Corruptions: Embedding-guided Pruning of Code Data},  <br>
&nbsp; author={Yu Yang and Aaditya K. Singh and Mostafa Elhoushi and Anas Mahmoud and Kushal Tirumala and Fabian Gloeckle and Baptiste Rozi√®re and Carole-Jean Wu and Ari S. Morcos and Newsha Ardalani}, <br>
&nbsp; year={2023}, <br>
&nbsp; eprint={2312.02418}, <br>
&nbsp; archivePrefix={arXiv}, <br>
&nbsp; primaryClass={cs.CL}, <br>
&nbsp; url={https://arxiv.org/abs/2312.02418},  <br>
}
</tt></p>
<script xml:space="preserve" language="JavaScript">
hidebib('yang2023scip');
</script>
</td></tr>
<!-- project end -->

<!-- project begin -->
<tr id="singh2022dixit"><td class="paperthumbcol">
<!-- project symbol -->
<img src="./assets/images/Dixit.png" class="paperthumb">
</td><td class="papertextcol">
<!-- title -->
<a href="https://arxiv.org/abs/2206.08349">
<papertitle>Know your audience: specializing grounded language models with listener subtraction</papertitle></a><br>
<!-- authors -->
<strong>Aaditya K. Singh</strong>,
<a href="https://scholar.google.com/citations?user=Un10q9gAAAAJ&hl=en">David Ding</a>,
<a href="https://scholar.google.co.uk/citations?user=h0Al1fcAAAAJ&hl=en">Andrew M. Saxe</a>,
<a href="https://scholar.google.co.uk/citations?user=4HLUnhIAAAAJ&hl=en">Felix Hill</a>,
<a href="https://scholar.google.com/citations?user=_N44XxAAAAAJ&hl=en">Andrew Kyle Lampinen</a><br>
<!-- publication status -->
<em>European chapter of the Association for Computational Linguistics (EACL)</em>, 2023<br>
<!-- links -->
<a href="https://arxiv.org/abs/2206.08349">arxiv</a> |
<a href="https://aclanthology.org/2023.eacl-main.279/">ACL Anthology</a> | 
<a href="https://x.com/Aaditya6284/status/1537807470093033472?lang=en">tweet</a> |
<a href="javascript:toggleAbsVsBib('singh2022dixit')" id="toggle">show bibtex</a>
<p></p>
<!-- project description -->
<p id="abstract">Effective communication requires adapting to the idiosyncrasies of each communicative context--such as the common ground shared with each partner. Humans demonstrate this ability to specialize to their audience in many contexts, such as the popular game Dixit. We take inspiration from Dixit to formulate a multi-agent image reference game where a (trained) speaker model is rewarded for describing a target image such that one (pretrained) listener model can correctly identify it among distractors, but another listener cannot. To adapt, the speaker must exploit differences in the knowledge it shares with the different listeners. We show that finetuning an attention-based adapter between a CLIP vision encoder and a large language model in this contrastive, multi-agent setting gives rise to context-dependent natural language specialization from rewards only, without direct supervision.</p>
<!-- bibtex -->
<p id="bibtex"><tt>
@inproceedings{singh-etal-2023-know, <br>
&nbsp; title = "Know your audience: specializing grounded language models with listener subtraction", <br>
&nbsp; author = "Singh, Aaditya K and Ding, David and Saxe, Andrew and Hill, Felix and Lampinen, Andrew", <br>
&nbsp; editor = "Vlachos, Andreas and Augenstein, Isabelle", <br>
&nbsp; booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics", <br>
&nbsp; month = may, <br>
&nbsp; year = "2023", <br>
&nbsp; address = "Dubrovnik, Croatia", <br>
&nbsp; publisher = "Association for Computational Linguistics", <br>
&nbsp; url = "https://aclanthology.org/2023.eacl-main.279", <br>
&nbsp; doi = "10.18653/v1/2023.eacl-main.279", <br>
&nbsp; pages = "3884--3911", <br>
}
</tt></p>
<script xml:space="preserve" language="JavaScript">
hidebib('singh2022dixit');
</script>
<p>Our <a href="https://arxiv.org/abs/2107.14795">Perceiver IO</a>-inspired cross-attention adapter was used by <a href="https://aclanthology.org/2023.eacl-main.185/">concurrent work</a> and shown to be generally useful for image captioning.</p>
</td></tr>
<!-- project end -->

<tr><td width="27%">
<heading2><i>Neuroscience</i></heading2>
</td/></tr>

<!-- project begin -->
<tr id="wang2024braintreebank"><td class="paperthumbcol">
<!-- project symbol -->
<img src="./assets/images/braintreebank.png" class="paperthumb">
</td><td class="papertextcol">
<!-- title -->
<a href="https://braintreebank.dev/">
<papertitle>Brain Treebank: Large-scale intracranial recordings from naturalistic language stimuli</papertitle></a><br>
<!-- authors -->
<a href="https://scholar.google.com/citations?user=x51pv_8AAAAJ&hl=en">Christopher Wang*</a>,
<a href="https://scholar.google.com/citations?user=s28yMP0AAAAJ&hl=en">Adam Yaari*</a>,
<strong>Aaditya K. Singh</strong>,
<a href="https://scholar.google.com/citations?user=Or3MAdgAAAAJ&hl=en">Vighnesh Subramaniam</a>,
<a href="https://cbmm.mit.edu/about/people/rosenfarb">Dana Rosenfarb</a>,
Jan DeWitt,
<a href="https://scholar.google.com/citations?user=cwcxlLkAAAAJ&hl=en">Pranav Misra</a>,
<a href="https://scholar.google.ca/citations?user=CgGqzY8AAAAJ&hl=en">Joseph R. Madsen</a>,
<a href="https://scholar.google.ca/citations?user=9HauEfkAAAAJ&hl=en">Scellig Stone</a>,
<a href="https://scholar.google.com/citations?user=WxZ_6nsAAAAJ&hl=en">Gabriel Kreiman</a>,
<a href="https://scholar.google.com/citations?user=FdNuUb8AAAAJ&hl=en">Boris Katz</a>,
<a href="https://scholar.google.com/citations?user=9-TdgYMAAAAJ&hl=en">Ignacio Cases</a>,
<a href="https://scholar.google.com/citations?user=t1rjgHgAAAAJ&hl=en">Andrei Barbu</a>,
<br>
<!-- publication status -->
<em>Neural Information Processing Systems (NeurIPS)</em>, 2024 <award>(Oral)</award><br>
<!-- links -->
<a href="https://braintreebank.dev/">project page</a> |
<a href="https://nips.cc/virtual/2024/oral/98023">neurips</a> | 
<a href="https://braintreebank.dev/paper.pdf">pdf</a> |
<a href="javascript:toggleAbsVsBib('wang2024braintreebank')" id="toggle">show bibtex</a>
<p></p>
<!-- project description -->
<p id="abstract">We present the Brain Treebank, a large-scale dataset of electrophysiological neural responses, recorded from intracranial probes while 10 subjects watched one or more Hollywood movies. Subjects watched on average 2.6 Hollywood movies, for an average viewing time of 4.3 hours, and a total of 43 hours. The audio track for each movie was transcribed with manual corrections. Word onsets were manually annotated on spectrograms of the audio track for each movie. Each transcript was automatically parsed and manually corrected into the universal dependencies (UD) formalism, assigning a part of speech to every word and a dependency parse to every sentence. In total, subjects heard over 38,000 sentences (223,000 words), while they had on average 168 electrodes implanted. This is the largest dataset of intracranial recordings featuring grounded naturalistic language, one of the largest English UD treebanks in general, and one of only a few UD treebanks aligned to multimodal features. We hope that this dataset serves as a bridge between linguistic concepts, perception, and their neural representations. To that end, we present an analysis of which electrodes are sensitive to language features while also mapping out a rough time course of language processing across these electrodes. </p>
<!-- bibtex -->
<p id="bibtex"><tt>
@inproceedings{wang2024braintreebank, <br>
&nbsp; author = {Wang, Christopher and Yaari, Adam and Singh, Aaditya and Subramaniam, Vighnesh and Misra, Pranav and Madsen, Joseph and Stone, Scellig and Kreiman, Gabriel and Katz, Boris and Cases, Ignacio and Barbu, Andrei}, <br>
<!-- &nbsp; booktitle = {Advances in Neural Information Processing Systems}, <br> -->
<!-- &nbsp; editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine}, <br> -->
<!-- &nbsp; pages = {27801--27819}, <br> -->
<!-- &nbsp; publisher = {Curran Associates, Inc.}, <br> -->
&nbsp; title = {Brain Treebank: Large-scale intracranial recordings from naturalistic language stimuli}, <br>
<!-- &nbsp; url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/58692a1701314e09cbd7a5f5f3871cc9-Paper-Conference.pdf}, <br> -->
<!-- &nbsp; volume = {36}, <br> -->
&nbsp; year = {2024} <br>
}
</tt></p>
<script xml:space="preserve" language="JavaScript">
hidebib('wang2024braintreebank');
</script>
</td></tr>
<!-- project end -->

</table>
<!-- end of research section -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
<tr> <td> <br>
<p align="right"> <font size="2">
<a href="https://github.com/djstrouse/djstrouse.github.io">Good artists copy.</a>
</font> </p> </td> </tr> </table>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
    var pageTracker = _gat._getTracker("UA-7580334-1");
    pageTracker._trackPageview();
    } catch(err) {}
</script>
</td></tr></table></body>